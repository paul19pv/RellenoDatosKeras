{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# librerias para el procesamiento\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pylab import *\n",
    "# libreria para generar el escalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Librerias para red neuronal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.constraints import min_max_norm\n",
    "#librearia para calcular la correlacion\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Leer los datos de las estaciones\n",
    "\n",
    "def leer_archivo(archivo,todas_estaciones):\n",
    "    datos = pd.DataFrame()\n",
    "    # leer datos de cada hoja de excel y asignarlos en una variable\n",
    "    # index_col = ingresar el numero de columna de la fecha. Se cuenta desde cero. La columna fecha se convierte en el indice de la tabla\n",
    "    # parsedates = para el ejemplo debe ser True. Analiza si el indice de la tabla es una fecha\n",
    "    # sheet_name = Nombre de la hoja de calculo\n",
    "    \n",
    "    for i in todas_estaciones:\n",
    "        # asignar en una variable los datos de la estacion\n",
    "        datos_estacion = pd.read_excel(io=archivo,index_col=2,parsedates=True,sheet_name=i)\n",
    "        # convertir la columna fecha al formato fecha 'AAAA-MM-DD'\n",
    "        datos_estacion.index = pd.to_datetime(datos_estacion.index)\n",
    "        # ordenar los datos por la fecha, desde más antigua\n",
    "        datos_estacion['VALOR'].sort_index(inplace=True)\n",
    "        # unir los datos de las estaciones en un solo DataFrame\n",
    "    \n",
    "        \n",
    "        datos[i] = datos_estacion['VALOR']\n",
    "    #### Anexar la columna semana a la matriz de datos\n",
    "    \n",
    "    #crear una columna para extraer la semana del año\n",
    "    datos['date']=datos.index\n",
    "    #aplicar una función para extraer la semana del año. Los valores van del 1 al 51\n",
    "    datos['week'] = datos['date'].apply(lambda x: x.isocalendar()[1])\n",
    "    #borrar la columna creada\n",
    "    del datos['date']\n",
    "    \n",
    "    return datos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los vacios de las series de datos\n",
    "def analisis_vacios(datos,rango_analisis,todas_estaciones):\n",
    "    inicio = rango_analisis[0]\n",
    "    final = rango_analisis[1]\n",
    "    # Crear un DataFrame para revisar\n",
    "    resumen = pd.DataFrame()\n",
    "    #crear una matriz para almacenar el número de vacios\n",
    "    por_vacios = pd.Series()\n",
    "    len_datos = pd.Series()\n",
    "    num_vacios = pd.Series()\n",
    "\n",
    "    # isnull(matriz) = permite identificar los valores nulos en una matriz\n",
    "    # matriz.to_numpy.nonzero()[0] = obtener un array de los valores no tienen datos\n",
    "    # matriz.loc[filas,columnas] = permite extraer una matriz de datos [filas,columnas]\n",
    "    # len(matriz) = longitud de una array\n",
    "    for i in todas_estaciones:\n",
    "        num_vac = len(pd.isnull(datos.loc[inicio:final,i]).to_numpy().nonzero()[0])\n",
    "        por_vac = round((100*num_vac/len(datos[i])),2)\n",
    "        num_vacios[i] = num_vac\n",
    "        len_datos[i] = len(datos.loc[inicio:final,i])\n",
    "        por_vacios[i] = por_vac\n",
    "\n",
    "\n",
    "    # num_vacios['P120'] = len(pd.isnull(datosP120.loc[:,'VALOR']).to_numpy().nonzero()[0])\n",
    "    resumen['Porcentaje Vacios'] = por_vacios\n",
    "    resumen['Total de datos'] = len_datos\n",
    "    resumen['Numero vacios'] = num_vacios\n",
    "    print(resumen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llenar_serie(datos,rango_analisis,indices):\n",
    "    inicio = rango_analisis[0]\n",
    "    final = rango_analisis[1]\n",
    "    # Crear un dataframe para los datos a procesar\n",
    "    datos_procesar = pd.DataFrame()\n",
    "    # datos que van a entrar en la red neuronal\n",
    "    datos_procesar = datos.loc[inicio:final,:].copy()\n",
    "    \n",
    "    # completar los datos de las estaciones con los valores promedio\n",
    "    for i in indices:\n",
    "        datos_procesar[i] = datos_procesar[i].fillna(datos_procesar[i].mean())\n",
    "    return datos_procesar\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenar el modelo para obtener los datos de predición\n",
    "def entrenar_modelo(datos,datos_procesar,estaciones_train,rango_analisis):\n",
    "    inicio = rango_analisis[0]\n",
    "    final = rango_analisis[1]\n",
    "    \n",
    "    #### Selección de datos para ingresar en el proceso\n",
    "    # est1 y est2 son las estaciones mas completas\n",
    "    est1 = estaciones_train[0]\n",
    "    est2 = estaciones_train[1]\n",
    "    # est3 estación a completar con la red neuronal\n",
    "    est3 = estaciones_train[2]\n",
    "    \n",
    "    #### Asignar valores a las variables de entrenamiento\n",
    "    \n",
    "    # En la variable X_train van los datos de la estaciones más completas y el valor de semana\n",
    "    X_train = datos_procesar.loc[inicio:final,[est1,est2,'week']].astype(np.float32).values\n",
    "    # En la varianle y_train va los datos de la estación a rellenar.\n",
    "    y_train = datos_procesar.loc[inicio:final,est3].astype(np.float32).values\n",
    "\n",
    "    \n",
    "    #obtener el valor maximo de la serie\n",
    "    \n",
    "    maximo = y_train.max()\n",
    "    minimo = y_train.min()\n",
    "    print(\"Valor Máximo Datos Originales\",maximo)\n",
    "    print(\"Valor Mínimo Datos Originales\",minimo)\n",
    "\n",
    "    #### Estandarizar la serie de datos eliminando la media y escalando a la varianza de la unidad\n",
    "    \n",
    "    # Definir el escalar \n",
    "    #scaler = StandardScaler().fit(X_train)\n",
    "    # Cambiar la serie transformando la serie a valores entre -1 a 1\n",
    "    #X_train = scaler.transform(X_train)\n",
    "    \n",
    "    X_train = get_escalar(X_train)\n",
    "    \n",
    "    #### Entrenamiento del modelo\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(12, activation='relu', input_shape=(3,),kernel_constraint=min_max_norm(min_value=minimo, max_value=maximo)))\n",
    "    model.add(Dense(8, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "                   \n",
    "    model.fit(X_train, y_train,epochs=300,verbose=0)\n",
    "    \n",
    "    # cargar en una variable los datos que arroja la red reuronal\n",
    "    y_pred = model.predict(X_train)\n",
    "    \n",
    "    maximo = y_pred.max()\n",
    "    minimo = y_pred.min()\n",
    "    print(\"Valor Máximo Datos Simulación\",maximo)\n",
    "    print(\"Valor Mínimo Datos Simulación\",minimo)\n",
    "    \n",
    "    # calcular la correlacion entre los datos originales y simulados\n",
    "    datos_correlacion = pd.DataFrame()\n",
    "    datos_correlacion['Original'] = datos.loc[inicio:final,est3].copy()\n",
    "    datos_correlacion['Simulado'] = y_pred\n",
    "    corr = datos_correlacion.corr()\n",
    "    print(\"Valor de Correlacion\",corr.at['Original', 'Simulado'])\n",
    "    \n",
    "    #graficar los datos de predicción\n",
    "    plot(datos_procesar.index,y_pred,label='Simulado')\n",
    "    #graficar los datos de est3\n",
    "    datos.loc[inicio:final,est3].plot(figsize=(16, 4),label='Original'); plt.legend(loc='best')    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datos_simulados(model,datos,estaciones_train,rango_analisis):\n",
    "    inicio = rango_analisis[0]\n",
    "    final = rango_analisis[1]\n",
    "    \n",
    "    # matriz para los datos simulados\n",
    "    datos_simulados = pd.Series()  \n",
    "    \n",
    "    #### Selección de datos para ingresar en el proceso\n",
    "    # est1 y est2 son las estaciones mas completas\n",
    "    est1 = estaciones_train[0]\n",
    "    est2 = estaciones_train[1]\n",
    "    # est3 estación a completar con la red neuronal\n",
    "    est3 = estaciones_train[2]\n",
    "    # Obtener el rango de datos que esta con vacios\n",
    "    rango = pd.isnull(datos.loc[inicio:final,est3]).to_numpy().nonzero()[0]\n",
    "    if len(rango)>0:\n",
    "        # Obtener los valores de est1, est2 basado en el rango de vacios de est3\n",
    "        X_missing = datos.loc[inicio:final,[est1,est2,'week']].iloc[rango].astype(np.float32).values\n",
    "\n",
    "        X_missing = get_escalar(X_missing)\n",
    "\n",
    "        #X_missing[:10]\n",
    "\n",
    "        # obtener los valores simulados con base al entrenamiento del modelo\n",
    "        num_datos = len (X_missing)\n",
    "        y_missing = model.predict(X_missing)\n",
    "        y_missing = y_missing.reshape([num_datos]).tolist()\n",
    "\n",
    "        datos_simulados = datos.loc[inicio:final,est3].copy()\n",
    "        # agregar los datos simulados a la estación P043\n",
    "        datos_simulados.iloc[rango]=y_missing\n",
    "    \n",
    "    return datos_simulados\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_escalar(matriz):\n",
    "    escalar = pd.DataFrame()\n",
    "    # Definir el escalar \n",
    "    scaler = StandardScaler().fit(matriz)\n",
    "    # Cambiar la serie transformando la serie a valores entre -1 a 1\n",
    "    escalar = scaler.transform(matriz)\n",
    "    \n",
    "    return escalar\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_correlacion(datos):\n",
    "    # calcular la correlación de los datos\n",
    "    corr = datos.corr()\n",
    "    #corr = datos.corr()\n",
    "    # generar la maskara para el grafico\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # relación de aspecto del grafico\n",
    "    plt.figure(figsize=(7,5))\n",
    "    #grafico de correlación\n",
    "    sns.heatmap(corr, annot=True)\n",
    "    #Apply xticks\n",
    "    plt.xticks(range(len(corr.columns)+1), corr.columns);\n",
    "    #Apply yticks\n",
    "    plt.yticks(range(len(corr.columns)+1), corr.columns)\n",
    "    #show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
